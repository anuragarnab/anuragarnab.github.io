<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>On the Robustness of Semantic Segmentation Models to Adversarial Attacks</title>

    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/style.css" rel="stylesheet">

    <!-- Fontawesome -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">

    <!-- Favicon -->
 	<link rel="icon" href="icons/oxford.ico" type="image/x-icon"/>
    <link rel="shortcut icon" href="icons/oxford.ico" type="image/x-icon"/>

	<!-- Google analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-83988063-1', 'auto');
		ga('send', 'pageview');
	</script>

  <!-- Mathjax -->
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>


  </head>
  <body>

    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">
			<img alt="Oxford" src="images/oxford_logo.png" class="img-responsive" style="height: 46px">
          </a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li><a href="index.html">Home</a></li>
            <li><a href="index.html#research">Research</a></li>
            <li><a href="publications.html">Publications</a></li>
            <li><a href="assets/CV_Anurag_Arnab.pdf" target="_blank" onclick="ga('send', 'event', 'File', 'download', this.href);">CV</a></li>
          </ul>

          <ul class="nav navbar-nav navbar-right">
            <li><a href="http://www.robots.ox.ac.uk/~tvg/">Torr Vision Group</a></li>
          </ul>            
            <!-- <img class="pull-right" src="./stuff/oxford_logo.png"> -->
        </div><!--/.nav-collapse -->
      </div>
    </nav>  	

    <div class="container">
	<div class="row">
		<div class="col-md-12">
			<!-- <div class="jumbotron"> -->
			<div class="page-header text-center">
				<br><br>
				<h1>
					On the Robustness of Semantic Segmentation Models to Adversarial Attacks <br>
            <small> 
              <a href="http://www.robots.ox.ac.uk/~aarnab/">Anurag Arnab</a>,
              <a href="http://www.miksik.co.uk/">Ondrej Miksik</a>
							and 
              <a href="https://scholar.google.com/citations?user=kPxa2w0AAAAJ&hl=en">Philip H.S. Torr</a> 
            </small>
				</h1>
			</div>
			<div class="row">
				<div class="col-md-9" style="border-right: 1px solid #EEE;">
					<p>

            <div class="row text-center">
                <div class="col-md-4">
                    <img class="img-responsive" src="projects/cvpr_2018/frankfurt_000000_000294_image_split.png" alt="Image">
                    Input image (perturbed half on right)
                </div>
              
                <div class="col-md-4">
                    <img class="img-responsive" src="projects/cvpr_2018/frankfurt_000000_000294_gtFine_color.png" alt="Ground truth">
                    Ground truth
                </div>
              
                <div class="col-md-4">
                    <img class="img-responsive" src="projects/cvpr_2018/frankfurt_000000_000294_pspnet_split.png" alt="PSPNet">
                    PSPNet
                </div>
            </div>               

            <div class="row text-center">
                <div class="col-md-4">
                    <img class="img-responsive" src="projects/cvpr_2018/frankfurt_000000_000294_dilated_split.png" alt="DilatedNet">
                    Dilated Net
                </div>
              
                <div class="col-md-4">
                    <img class="img-responsive" src="projects/cvpr_2018/frankfurt_000000_000294_icnet_split.png" alt="ICNet">
                    ICNet
                </div>
              
                <div class="col-md-4">
                    <img class="img-responsive" src="projects/cvpr_2018/frankfurt_000000_000294_crfrnn_split.png" alt="CRF as RNN">
                    CRF as RNN
                </div>
            </div>               

            <br>
            <strong>Abstract: </strong>
            
            Deep Neural Networks (DNNs) have been demonstrated to perform exceptionally well on most recognition tasks such as image classification and segmentation. However, they have also been shown to be vulnerable to adversarial examples. This phenomenon has recently attracted a lot of attention but it has not been extensively studied on multiple, large-scale datasets and complex tasks such as semantic segmentation which often require more specialised networks with additional components such as CRFs, dilated convolutions, skip-connections and multiscale processing. 
            <br>
            In this paper, we present what to our knowledge is the first rigorous evaluation of adversarial attacks on modern semantic segmentation models, using two large-scale datasets. We analyse the effect of different network architectures, model capacity and multiscale processing, and show that many observations made on the task of classification do not always transfer to this more complex task. Furthermore, we show how mean-field inference in deep structured models and multiscale processing naturally implement recently proposed adversarial defenses. Our observations will aid future efforts in understanding and defending against adversarial examples. Moreover, in the shorter term, we show which segmentation models should currently be preferred in safety-critical applications due to their inherent robustness.
            <hr>
        
				</div>
				<div class="col-md-3">
					 
					 <a href="projects/cvpr_2018/Arnab_CVPR_2018.pdf" type="button" class="btn btn-block btn-primary">
					 	Download Paper <i class="fa fa-file-pdf-o"></i>
           </a>
					 <a href="projects/cvpr_2018/Arnab_CVPR_2018_extended.pdf" type="button" class="btn btn-block btn-primary">
					 	Download Paper (extended version) <i class="fa fa-file-pdf-o"></i>
           </a>
           <a href="https://github.com/hmph/adversarial-attacks" type="button" class="btn btn-block btn-primary">
            Download Code <i class="fa fa-github fa-lg"></i>
          </a>
           <!--
					 <a href="projects/cvpr_2017/Instances_CVPR_slides.pptx" type="button" class="btn btn-primary btn-block">
						Download Slides <i class="fa fa-file-powerpoint-o"></i>
					</a> -->
          <a href="projects/cvpr_2018/Poster_CVPR_2018.pdf" type="button" class="btn btn-primary btn-block">
						Download Poster <i class="fa fa-file-pdf-o"></i>
          </a>
          

				</div>
			</div>
    </div>
	</div>
</div>

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/scripts.js"></script>
  </body>
</html>
